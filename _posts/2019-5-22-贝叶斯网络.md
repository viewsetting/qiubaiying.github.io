---
layout:     post
title:     贝叶斯网络
subtitle:   Bayesian Network
date:       2019-5-22
author:     viewsetting
header-img: img/post-adjani.jpg
catalog: true
tags:
    - Notes
    - Machine Learning
---

# 动机

对于一个全联合的概率分步，如果随机变量为n个二值变量，那么其计算的复杂度就为$ O(2^{n-1})$ ,很显然如果维数稍稍增大的话，计算量便会非常恐怖。因此为了减少概率的计算数量，我们可以利用条件独立关系减少相关的概率。贝叶斯网络便是通过一个有向图的结构，表示变量间的条件独立关系。

Bayesian Network的定义是：

1. 每个节点就是一个随机变量，可以是连续也可以为离散
2. DAG，即有向无环图。如果$X\rightarrow Y$ ,则称$X$为$Y$的父节点
3. 每个节点$X_i$都有一个条件概率分布$P(X_i\vert Parents(X_i))$ 

## 分解因式

对于一个联合概率分布，我们有：

$P(x_1,x_2,...,x_n)=\prod_{k=1}^nP(x_k\vert x_{1:k-1})$

利用BN分解因式得到：

$P(x_1,x_2,...,x_n)=\prod_{i=1}^pP(x_i\vert x_{parent(i)})$

# 三种分解后的模式

### head2head

Union Distribution:

$P(a,b,c)=P(a)P(b\vert a)P(c\vert a,b)$

Bayesian Rule:

$P(a,b,c)=P(a)P(b\vert a)P(c\vert a)$

联立得到：

$P(c\vert a,b)=P(c\vert a)$,故在给定$P(a)$时，$b \perp c \vert  a$ 即$b$和$c$在给定$a$的概率时（观测到$a$的概率）条件独立。

### head2tail

形如链式。

给定中间的$b$的概率之后：

根据Union Distribution , $P(a,b,c)=P(a)P(b\vert a)P(c\vert a,b)$

根据贝叶斯公式：$P(a,b,c)=P(a)P(b\vert a)P(c\vert b)$

得到$c \perp a \vert  b$

### tail2tail

如果$c$未被观测，那么

根据联合概率：

$P(a,b,c)=P(a)P(b\vert a)P(c\vert a,b)$

根据贝叶斯：

$P(a,b,c)=P(a)P(b)P(c\vert a,b)$

那么：$P(b\vert a)=P(b)$ 也就是$a$和$b$无条件独立

# 应用

### D-Separation

If we have 3 sets $X_A,X_B,X_C$, all of which are consists of randomized varieties and the intersections between each are empty sets.

Assume that there is a path between nodes of $X_A$to those of $X_C$,all of nodes on that path should be in $X_B$.

If there is $X_B\rightarrow X_A$ and $X_B\rightarrow X_C$ ，with $X_B$ be observed, we will have the conclusion $X_A\perp X_C \vert X_B $



### Markov Blanket

可以证明，在一个贝叶斯网络中，有三部分与给定的某个随机变量$x_i$有关：

- $x_i$的父节点
- $x_i$的子节点
- $x_i$所有子节点的所有除了$x_i$的父节点

换言之，如果观测了一个随机变量的$Markov\quad Blanket$之后，该贝叶斯网络的其他随机变量与$x_i$条件独立。可以看出，$x_i$的父节点阻隔了更高阶的父节点与$x_i$的关联，即阻隔head2tail模式，同理$x_i$的子节点也阻隔了$x_i$与其高阶子节点的关系（和head2tail的链式结构与head2head的结构兼有）。最后，由于$x_i$的子节点均已被观测，那么此时$x_i$的子节点的父节点均与$x_i$和它自己的公共子节点构成tail2tail结构，此时二者失去独立性，故这些子节点的其余父节点也要纳入Markov Blanket中。

即：

计算$x_i$在$x_1...x_{i-1}x_{i+1}...x{n}$下的概率$P(x_i\vert x_1...x_{i-1}x_{i+1}...x{n})=P(X)/P(x_{-i})$

继续展开求边缘概率：

$=P(X)/ \int_{x_i} P(x) dx_i $

再用贝叶斯：

$$=\frac{\prod_{j=1}^{n}P(x_j \vert x_{pa(j)})}{\int_{x_i} \prod_{j=1}^{n}P(x_j\vert x_{pa(j)})dx_i} $$

然后自然$x_i$的MB部分在公式中成为常量提出，上下约分 。



# 贝叶斯网络分类

### 单一模型

**Naive Bayes**, 基于朴素贝叶斯假设：$P(x\vert y)=\prod_{i=1}^nP(x_i\vert y=1)$

即：当$y$被观测到为确定时，所有的随机变量$x$都互相独立，用于分类问题，维数等于分类数。

### 混合模型

**Gaussian Mixed Model**，可用于Clustering。

引入一个离散值$Z=1,2,...,k$  , 记$X\vert Z\sim N(\mu,\Sigma)$ ,$X$与$Z$满足参数为$(\mu,\Sigma)$的参数分布。

### 基于时间的模型

**Markov Chain**

**HMM** , 隐马尔科夫模型。

**Gaussian Process** 高斯过程，无限维的Gaussian Distribution

**LDS** 线性动态系统 如：Kalman Filter

**Particle Filter** 非连续非线性系统

### 连续模型

**Gaussian Bayesian Network**